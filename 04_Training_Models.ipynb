{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Models\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In chapters 2 and 3 we used a lot of different models without actually understanding what was going on under the hood, i.e. we had no clue about the underlying mathematics used for creating and training such algorithms. Well, this chapter will expose some of the most essentials and basic algorithms in machine learning, namely: \n",
    "+ linear regression\n",
    "+ polynomial regression\n",
    "+ logistic regression (used for classification)\n",
    "+ softmax regression (also used for classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression is model based on the computation of the weighted sum of all the input features: \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "where: \n",
    "\n",
    "+ $\\theta_k$ are the weights attributed to each feature $x_k$.\n",
    "+ $\\hat{y}$ is the predicted target variable (the hat means prediction).\n",
    "+ $\\theta_0$ is the bias\n",
    "\n",
    "objective: the goal of the linear regression is to *find* the correct values for all the parameters $\\theta_k$, such that we realize the best predicitions $\\hat{y}$.\n",
    "\n",
    "**Matrix Notation**\n",
    "\n",
    "In general, we prefer to use linear algebra to write these equations as it is more concise: \n",
    "$$\n",
    "\\hat{y} = \\theta^T x\n",
    "$$\n",
    "\n",
    "where: $\\theta^T = [\\theta_0, \\theta_1, (\\cdots), \\theta_n]$.  \n",
    "note that $x_0 = 1$ to preserve the bias.\n",
    "\n",
    "**Training the Model**\n",
    "\n",
    "To train the model, we want to minimize the errors we are making, i.e. we want to minimizes the distance between $y$, the target variable, and $\\hat{y}$, the predicted target variable. How can we do this? Well, once we have the correct intuition, the answer is quite straightforward. Since we aim to minimize the divergence of $\\hat{y}$ from $y$, we select a **loss function** (e.g. the MSE) and we find the vector (the parameters or weights) that minimizes it. Therefore, the optimization problem we face is the following (the MSE is convenient since it is a convex function): \n",
    "$$\n",
    "MSE(\\mathbf{X}, h_\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (\\theta^Tx^{(i)} - y^{(i)})^2\n",
    "$$\n",
    "where $h_{\\theta}$ is the hypothesis for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The Normal Form Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solving the Optimization Problem**  \n",
    "\n",
    "To proceed, we first rewrite our problem in matrix notation: \n",
    "$$\n",
    "\\text{mse} = \\frac{1}{n} (\\mathbf{X}\\theta - y)^2\n",
    "$$\n",
    "where: \n",
    "+ $n$: number of observations \n",
    "+ $m$: number of features available for predictions\n",
    "+ $\\mathbf{X}$: feature matrix $(n \\times m)$\n",
    "+ $\\theta$: paramter vector $(m \\times 1)$ \n",
    "+ $y$: target variable vector $(n \\times 1)$\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} (X\\theta - y)^T (X\\theta - y)\n",
    "$$\n",
    "\n",
    "Expanding the expression:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\left( \\theta^T X^T X \\theta - y^T X \\theta - \\theta^T X^T y + y^T y \\right)\n",
    "$$\n",
    "\n",
    "Since $ (y^T X \\theta)^T = \\theta^T X^T y $, we have:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\left( \\theta^T X^T X \\theta - 2 \\theta^T X^T y + y^T y \\right)\n",
    "$$\n",
    "\n",
    "Gradient of MSE with respect to $ \\theta $:\n",
    "$$\n",
    "\\nabla_\\theta (\\text{MSE}) = \\frac{1}{n} \\left( 2 X^T X \\theta - 2 X^T y \\right)\n",
    "$$\n",
    "\n",
    "Setting the gradient to zero for optimization:\n",
    "$$\n",
    "\\frac{1}{n} \\left( 2 X^T X \\theta - 2 X^T y \\right) = 0\n",
    "$$\n",
    "\n",
    "simplifying:\n",
    "$$\n",
    "X^T X \\theta = X^T y\n",
    "$$\n",
    "\n",
    "Solution for $\\theta$:\n",
    "> $$\n",
    "> \\hat{\\theta} = (X^T X)^{-1} X^T y\n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **normal equation** or the closed form solution for finding the estimated parameters vector $\\hat{\\theta}$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
